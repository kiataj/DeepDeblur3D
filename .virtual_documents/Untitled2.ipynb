import os, math, warnings
from pathlib import Path
from typing import List, Optional, Tuple, Dict

import numpy as np
import pandas as pd
from tqdm import tqdm

try:
    import tifffile as tiff
except Exception as e:
    raise RuntimeError("This script requires 'tifffile'. Install: pip install tifffile") from e


# ---------- helpers ----------
def _list_slices(scan_dir: Path) -> List[Path]:
    """List 2D TIFF slices in natural order, ignoring proj_* and mask_*."""
    import re
    def natural_key(name: str):
        parts = re.split(r'(\d+)', name)
        parts[1::2] = [int(p) for p in parts[1::2]]
        return parts

    exts = (".tif", ".tiff")
    files = [p for p in scan_dir.iterdir()
             if p.is_file()
             and p.suffix.lower() in exts
             and not p.name.lower().startswith(("proj_", "mask_"))]
    files.sort(key=lambda p: natural_key(p.name))
    return files


def _estimate_lo_hi_from_sample(paths: List[Path],
                                sample_slices: int = 64,
                                percentiles: Tuple[float, float] = (1.0, 99.9)) -> Tuple[float, float]:
    """Robust lo/hi from a subset of slices to avoid reading entire volume."""
    if not paths:
        return 0.0, 1.0
    idx = np.linspace(0, len(paths)-1, num=min(len(paths), sample_slices), dtype=int)
    samp = []
    for i in idx:
        arr = tiff.imread(str(paths[i]))
        if arr.ndim != 2:
            raise ValueError(f"Slice {paths[i]} is not 2D, got shape {arr.shape}")
        samp.append(arr.astype(np.float32, copy=False))
    block = np.stack(samp, axis=0)
    lo, hi = np.percentile(block, percentiles).tolist()
    if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:
        lo, hi = float(block.min()), float(block.max())
    return float(lo), float(hi)


def _scale_to_u8(arr_f32: np.ndarray, lo: float, hi: float) -> np.ndarray:
    """Scale float32 array to uint8 using lo/hi, clipping to [0, 255]."""
    y = (arr_f32 - lo) / max(hi - lo, 1e-6)
    y = np.clip(y, 0.0, 1.0)
    return (y * 255.0 + 0.5).astype(np.uint8, copy=False)


# ---------- main converter ----------
def convert_manifest_sequences_to_single_tifs(
    manifest_path: str | Path,
    out_dir: str | Path,
    *,
    filter_query: Optional[str] = None,        # e.g. "n_slices >= 128 and size_8bit_GB < 6"
    id_prefix: str = "S",
    start_id: int = 1,
    overwrite: bool = False,
    sample_slices_for_range: int = 64,
    range_percentiles: Tuple[float, float] = (1.0, 99.9),
    bigtiff_threshold_gb: float = 3.5,         # switch to BigTIFF if projected size exceeds this
    write_compression: Optional[str] = None,   # e.g. "zlib" (slower, smaller). None = uncompressed
    excel_name: str = "single_tif_index.xlsx",
) -> Path:
    """
    Convert per-scan TIFF slice folders (from parquet manifest) into single 8-bit multi-page TIFFs.

    - Uses per-row lo/hi if present; otherwise estimates from a sample.
    - Writes an Excel mapping with scan_id, output path, and manifest fields merged.

    Returns: path to the Excel index file.
    """
    manifest_path = Path(manifest_path)
    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)

    # Load manifest
    if manifest_path.suffix.lower() == ".parquet":
        df = pd.read_parquet(manifest_path)
    else:
        df = pd.read_csv(manifest_path)

    if filter_query:
        try:
            df = df.query(filter_query, engine="python")
        except Exception as e:
            raise ValueError(f"Invalid filter_query: {filter_query}\n{e}")

    if df.empty:
        raise RuntimeError("No rows after filtering—nothing to convert.")

    # Required columns
    required = {"root_dir"}
    missing = required - set(df.columns)
    if missing:
        raise ValueError(f"Manifest is missing required columns: {missing}")

    # Prepare ID assignment
    next_id = int(start_id)
    records = []

    # Iterate rows (one scan per row -> scan folder with a sequence of tifs)
    for ridx, row in tqdm(df.reset_index(drop=True).iterrows(), total=len(df), desc="Converting scans"):
        scan_dir = Path(row["root_dir"])
        if not scan_dir.exists():
            warnings.warn(f"[skip] Missing scan folder: {scan_dir}")
            continue

        slices = _list_slices(scan_dir)
        if len(slices) == 0:
            warnings.warn(f"[skip] No tif slices in: {scan_dir}")
            continue

        # Shape check
        s0 = tiff.imread(str(slices[0]))
        if s0.ndim != 2:
            warnings.warn(f"[skip] First slice not 2D in: {scan_dir} (shape={s0.shape})")
            continue
        H, W = s0.shape
        D = len(slices)

        # Decide lo/hi
        lo = float(row["lo"]) if "lo" in row and pd.notna(row["lo"]) else None
        hi = float(row["hi"]) if "hi" in row and pd.notna(row["hi"]) else None
        if lo is None or hi is None:
            lo, hi = _estimate_lo_hi_from_sample(
                slices, sample_slices=sample_slices_for_range, percentiles=range_percentiles
            )

        # Make output filename
        scan_id = f"{id_prefix}{next_id:04d}"
        out_path = out_dir / f"{scan_id}.tif"
        next_id += 1

        if out_path.exists() and not overwrite:
            warnings.warn(f"[skip] Exists: {out_path} (use overwrite=True to rewrite)")
            # still record mapping
            rec = dict(row)
            rec.update({
                "scan_id": scan_id,
                "output_tif": str(out_path),
                "out_H": int(H), "out_W": int(W), "out_D": int(D),
                "used_lo": lo, "used_hi": hi,
                "status": "skipped_exists",
            })
            records.append(rec)
            continue

        # Predict output size to decide BigTIFF (8-bit, no compression)
        projected_gb = (D * H * W) / 1e9  # bytes in GB if uint8, single channel, no compression
        bigtiff_flag = projected_gb > bigtiff_threshold_gb

        # Write multi-page TIFF (streamed, slice-by-slice)
        try:
            with tiff.TiffWriter(str(out_path), bigtiff=bigtiff_flag) as tw:
                for si, spath in enumerate(slices):
                    arr = tiff.imread(str(spath)).astype(np.float32, copy=False)
                    u8 = _scale_to_u8(arr, lo, hi)  # uint8
                    tw.write(
                        u8,
                        photometric="minisblack",
                        compression=write_compression,  # None, "zlib", "lzma", "zstd" (if built), etc.
                        contiguous=False,
                    )
        except Exception as e:
            warnings.warn(f"[error] Failed writing {out_path}: {e}")
            continue

        rec = dict(row)
        rec.update({
            "scan_id": scan_id,
            "output_tif": str(out_path),
            "out_H": int(H), "out_W": int(W), "out_D": int(D),
            "used_lo": lo, "used_hi": hi,
            "status": "ok",
        })
        records.append(rec)

    # Build Excel index with all captured fields
    if not records:
        raise RuntimeError("No scans converted—check your filter or manifest.")

    df_out = pd.DataFrame(records)
    excel_path = out_dir / excel_name
    with pd.ExcelWriter(excel_path, engine="xlsxwriter") as xw:
        df_out.to_excel(xw, index=False, sheet_name="index")

    print(f"\nDone. Wrote {len(df_out[df_out['status']=='ok'])} scans to: {out_dir}")
    print(f"Excel index: {excel_path}")
    return excel_path



excel_index = convert_manifest_sequences_to_single_tifs(
    manifest_path=r"C:\Users\taki\DeepDeBlur3D\manifest.parquet",
    out_dir=r"T:\users\taki\Dataset",              # local SSD recommended
    filter_query="size_8bit_GB < 0.4",  # optional
    id_prefix="S",
    start_id=1,
    overwrite=False,
    sample_slices_for_range=64,                   # for robust lo/hi estimate per scan
    range_percentiles=(1.0, 99.9),
    bigtiff_threshold_gb=3.5,                     # auto BigTIFF for huge volumes
    write_compression=None,                       # or "zlib" if you want smaller files (slower)
    excel_name="index_single_8bit.xlsx",
)



# convert_manifest_to_excel.py
import os
from pathlib import Path
import pandas as pd

def convert_manifest_to_excel(
    manifest_path: str | os.PathLike,
    out_xlsx: str | os.PathLike,
    filter_query: str | None = None,
    select_cols: list[str] | None = None,
    sort_by: list[str] | None = None,
) -> Path:
    """
    Convert a deblur3d manifest (parquet/csv) to an Excel workbook.
    
    - filter_query: optional pandas query string, e.g. "n_slices>=128 and size_8bit_GB<6"
    - select_cols: optional list of columns to keep (in this order)
    - sort_by:     optional list of columns to sort by (ascending)
    """
    mp = Path(manifest_path)
    if not mp.exists():
        raise FileNotFoundError(f"Manifest not found: {mp}")

    # Load
    if mp.suffix.lower() == ".parquet":
        df = pd.read_parquet(mp)
    else:
        df = pd.read_csv(mp)

    # Filter
    if filter_query:
        df = df.query(filter_query, engine="python").copy()

    if df.empty:
        raise RuntimeError("No rows after filtering; nothing to write.")

    # Add derived sizes if available (bytes → GB for readability)
    # Keep existing columns if already present.
    if {"n_slices","H","W"}.issubset(df.columns) and "size_8bit_GB" not in df.columns:
        df["size_8bit_GB"] = (df["n_slices"].astype("float64") * df["H"] * df["W"]) / 1e9

    if "xml_bytes_8bit_from_xml" in df.columns and "size_8bit_GB_xml" not in df.columns:
        df["size_8bit_GB_xml"] = df["xml_bytes_8bit_from_xml"] / 1e9

    # Column selection
    if select_cols:
        keep = [c for c in select_cols if c in df.columns]
        df = df[keep]

    # Sorting
    if sort_by:
        sort_cols = [c for c in sort_by if c in df.columns]
        if sort_cols:
            df = df.sort_values(sort_cols).reset_index(drop=True)

    # Summary sheet
    summary = {
        "rows": [len(df)],
        "unique_projects": [df["project_path"].nunique() if "project_path" in df.columns else None],
        "total_size_8bit_GB": [df["size_8bit_GB"].sum() if "size_8bit_GB" in df.columns else None],
        "total_size_8bit_GB_xml": [df["size_8bit_GB_xml"].sum() if "size_8bit_GB_xml" in df.columns else None],
        "mean_H": [df["H"].mean() if "H" in df.columns else None],
        "mean_W": [df["W"].mean() if "W" in df.columns else None],
        "mean_n_slices": [df["n_slices"].mean() if "n_slices" in df.columns else None],
    }
    df_summary = pd.DataFrame(summary)

    # Write Excel
    out = Path(out_xlsx)
    out.parent.mkdir(parents=True, exist_ok=True)
    with pd.ExcelWriter(out, engine="xlsxwriter") as writer:
        df.to_excel(writer, sheet_name="Scans", index=False)
        df_summary.to_excel(writer, sheet_name="Summary", index=False)

        # Optional: add a simple table style
        wb  = writer.book
        ws1 = writer.sheets["Scans"]
        ws2 = writer.sheets["Summary"]
        if len(df) > 0:
            ws1.add_table(0, 0, len(df), max(0, len(df.columns)-1),
                          {"name": "ScansTable", "columns": [{"header": c} for c in df.columns]})
        ws2.add_table(0, 0, len(df_summary), max(0, len(df_summary.columns)-1),
                      {"name": "SummaryTable", "columns": [{"header": c} for c in df_summary.columns]})
        # Autofit-ish: set column widths
        for i, c in enumerate(df.columns):
            width = min(60, max(10, int(df[c].astype(str).str.len().quantile(0.95)) + 2))
            ws1.set_column(i, i, width)
        for i, c in enumerate(df_summary.columns):
            ws2.set_column(i, i, max(12, len(c) + 2))

    print(f"Saved Excel → {out}")
    return out

# ===== example usage =====
if __name__ == "__main__":
    manifest = r"C:\Users\taki\DeepDeBlur3D\manifest.parquet"
    out_xlsx = r"C:\Users\taki\DeepDeBlur3D\manifest_export.xlsx"

    convert_manifest_to_excel(
        manifest_path=manifest,
        out_xlsx=out_xlsx,
        filter_query=None,  # e.g. "n_slices>=128 and size_8bit_GB<6"
        select_cols=[
            "volume_id","project_path","root_dir","n_slices","H","W","dtype",
            "lo","hi","spacing_y","spacing_x","size_8bit_GB","size_8bit_GB_xml"
        ],
        sort_by=["project_path","root_dir"]
    )



