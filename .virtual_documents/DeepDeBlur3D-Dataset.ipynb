from deblur3d.data.tiff_sequence import TiffDataset
from deblur3d.transforms import GaussianIsoBlurCPUTransform  # your CPU blur
import pandas as pd

blur_tf = GaussianIsoBlurCPUTransform(
    fwhm_range=(6, 12), radius_mult=3,
    add_noise=True, poisson_gain_range=(400, 900),
    read_noise_std_range=(0.004, 0.012)
)

ds_train = TiffDataset(
    manifest_path="manifest.parquet",
    split=None,                      # or 'train' if you add a split column later
    patch_size=(64, 256, 256),
    blur_transform=blur_tf,
    balance="volume",
    samples_per_epoch= len(pd.read_parquet("manifest.parquet")) * 8  # optional
)

from torch.utils.data import DataLoader
loader = DataLoader(ds_train, batch_size=2, shuffle=False, num_workers=4, pin_memory=True)
sharp, blurred = next(iter(loader))
print(sharp.shape, blurred.shape)  # (B, D, H, W)



import matplotlib.pyplot as plt
import torch

# sharp, blurred from your DataLoader
# shapes: (B, D, H, W) torch.float32 in [0,1]

# pick first item in batch; move to CPU numpy
s = sharp[0].detach().cpu()
b = blurred[0].detach().cpu()

# central slice index along depth
z = s.shape[0] // 2

# consistent contrast limits across both
vmin = float(torch.minimum(s.min(), b.min()))
vmax = float(torch.maximum(s.max(), b.max()))

fig, axes = plt.subplots(1, 2, figsize=(10, 5), constrained_layout=True)

axes[0].imshow(s[z].numpy(), cmap='gray', vmin=vmin, vmax=vmax)
axes[0].set_title(f"Sharp (z={z})")
axes[0].axis('off')

axes[1].imshow(b[z].numpy(), cmap='gray', vmin=vmin, vmax=vmax)
axes[1].set_title(f"Blurred (z={z})")
axes[1].axis('off')

plt.show()



import os, pandas as pd
from pathlib import Path

man_path = "manifest.parquet"  # or .csv
m = pd.read_parquet(man_path) if man_path.endswith(".parquet") else pd.read_csv(man_path)

print("Rows in manifest:", len(m))
display(m.head(10))

# Basic stats
cols = ["n_slices","H","W","dtype","lo","hi","root_dir"]
display(m[cols].describe(include="all"))

# Any paths that no longer exist?
missing = m[~m["root_dir"].map(os.path.exists)]
print("Missing on disk:", len(missing))
if len(missing):
    display(missing[["root_dir","n_slices","H","W","dtype"]].head(10))



from itertools import islice
log_path = Path(man_path).with_suffix(".index_log.jsonl")
if log_path.exists():
    print("Index log (first 20 lines):")
    with open(log_path, "r", encoding="utf-8") as f:
        for line in islice(f, 20):
            print(line.strip())



import pandas as pd

m = pd.read_parquet("manifest.parquet")   # or read_csv(...)
print("Rows:", len(m))
print("Unique root_dir:", m["root_dir"].nunique())  # should match rows

# What the dataset will actually use (skips missing dirs)
from deblur3d.data import TiffDataset
ds = TiffDataset("data/manifest.parquet", patch_size=(64,256,256))
print("Usable volumes in dataset:", len(ds.vols))






import os
from pathlib import Path
import pandas as pd

# --- config ---
MANIFEST = "manifest.parquet"  # or .csv
METHOD   = "exact"                  # "exact" or "estimate"

# map common dtypes to bytes-per-pixel (extend if needed)
BPP = {
    "uint8":1, "int8":1,
    "uint16":2, "int16":2,
    "uint32":4, "int32":4,
    "float32":4, "float64":8,
    "double":8, "single":4,
}

def _load_manifest(path: str) -> pd.DataFrame:
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(p)
    if p.suffix.lower() == ".parquet":
        return pd.read_parquet(p)
    return pd.read_csv(p)

def _bytes_to_tb_tib(nbytes: int) -> tuple[float, float]:
    tb  = nbytes / 1_000_000_000_000  # decimal TB
    tib = nbytes / (1024**4)          # binary TiB
    return tb, tib

def _iter_slices(folder: Path):
    for f in folder.iterdir():
        if not f.is_file():
            continue
        n = f.name.lower()
        if not (n.endswith(".tif") or n.endswith(".tiff")):
            continue
        if n.startswith("proj_") or n.startswith("mask_"):
            continue
        yield f

def dataset_size_exact(df: pd.DataFrame) -> int:
    total = 0
    missing = 0
    for root in df["root_dir"]:
        d = Path(root)
        if not d.exists():
            missing += 1
            continue
        for f in _iter_slices(d):
            try:
                total += f.stat().st_size
            except OSError:
                pass
    if missing:
        print(f"Note: {missing} folders listed in manifest are missing on disk.")
    return total

def dataset_size_estimate(df: pd.DataFrame) -> int:
    # needs columns: n_slices, H, W, dtype
    if not {"n_slices","H","W","dtype"}.issubset(df.columns):
        raise ValueError("Manifest is missing one of: n_slices, H, W, dtype (required for estimate).")
    def bpp_of(dt: str) -> int:
        s = str(dt).lower()
        for k, v in BPP.items():
            if k in s:
                return v
        # fallback if unknown dtype
        return 2
    sizes = []
    for _, r in df.iterrows():
        n = int(r["n_slices"]); H = int(r["H"]); W = int(r["W"])
        bpp = bpp_of(r["dtype"])
        sizes.append(n * H * W * bpp)
    return int(sum(sizes))

# ---- run ----
df = _load_manifest(MANIFEST)

if METHOD == "exact":
    total_bytes = dataset_size_exact(df)
else:
    total_bytes = dataset_size_estimate(df)

tb, tib = _bytes_to_tb_tib(total_bytes)
print(f"Volumes in manifest: {len(df)}")
print(f"Total size ≈ {total_bytes:,} bytes  →  {tb:.3f} TB  ({tib:.3f} TiB)")




