{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a99b7b66-b32a-421e-a8a2-51c7d8cd105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    import tifffile as tiff\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"This script requires 'tifffile'. Install: pip install tifffile\") from e\n",
    "\n",
    "\n",
    "# -------------------- helpers --------------------\n",
    "def _list_slices(scan_dir: Path) -> List[Path]:\n",
    "    \"\"\"\n",
    "    List 2D TIFF slices in natural order, ignoring files starting with 'proj_' or 'mask_' (case-insensitive).\n",
    "    \"\"\"\n",
    "    import re\n",
    "    def natural_key(name: str):\n",
    "        parts = re.split(r'(\\d+)', name)\n",
    "        parts[1::2] = [int(p) for p in parts[1::2]]\n",
    "        return parts\n",
    "\n",
    "    exts = (\".tif\", \".tiff\")\n",
    "    files = [p for p in scan_dir.iterdir()\n",
    "             if p.is_file()\n",
    "             and p.suffix.lower() in exts\n",
    "             and not p.name.lower().startswith((\"proj_\", \"mask_\"))]\n",
    "    files.sort(key=lambda p: natural_key(p.name))\n",
    "    return files\n",
    "\n",
    "\n",
    "def _estimate_lo_hi_from_sample(paths: List[Path],\n",
    "                                sample_slices: int = 64,\n",
    "                                percentiles: Tuple[float, float] = (1.0, 99.9)) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Robust lo/hi from a subset of slices to avoid reading entire volume.\n",
    "    \"\"\"\n",
    "    if not paths:\n",
    "        return 0.0, 1.0\n",
    "    idx = np.linspace(0, len(paths) - 1, num=min(len(paths), sample_slices), dtype=int)\n",
    "    samp = []\n",
    "    for i in idx:\n",
    "        arr = tiff.imread(str(paths[i]))\n",
    "        if arr.ndim != 2:\n",
    "            raise ValueError(f\"Slice {paths[i]} is not 2D, got shape {arr.shape}\")\n",
    "        samp.append(arr.astype(np.float32, copy=False))\n",
    "    block = np.stack(samp, axis=0)\n",
    "    lo, hi = np.percentile(block, percentiles).tolist()\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:\n",
    "        lo, hi = float(block.min()), float(block.max())\n",
    "    return float(lo), float(hi)\n",
    "\n",
    "\n",
    "def _scale_to_u8(arr_f32: np.ndarray, lo: float, hi: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Scale float32 array to uint8 using lo/hi, clipping to [0, 255].\n",
    "    \"\"\"\n",
    "    y = (arr_f32 - lo) / max(hi - lo, 1e-6)\n",
    "    y = np.clip(y, 0.0, 1.0)\n",
    "    return (y * 255.0 + 0.5).astype(np.uint8, copy=False)\n",
    "\n",
    "\n",
    "def _pool2d_sum_u8(u8: np.ndarray, factor: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sum non-overlapping (factor x factor) blocks on a uint8 2D array → uint32.\n",
    "    Used to accumulate spatial block sums before dividing to get mean.\n",
    "    \"\"\"\n",
    "    if factor <= 1:\n",
    "        return u8.astype(np.uint32, copy=False)\n",
    "    H, W = u8.shape\n",
    "    HH = (H // factor) * factor\n",
    "    WW = (W // factor) * factor\n",
    "    if HH == 0 or WW == 0:\n",
    "        return u8.astype(np.uint32, copy=False)\n",
    "    x = u8[:HH, :WW].reshape(HH // factor, factor, WW // factor, factor)\n",
    "    return x.sum(axis=(1, 3), dtype=np.uint32)\n",
    "\n",
    "\n",
    "def _downsample3d_streaming_write(\n",
    "    slices: List[Path],\n",
    "    tw,\n",
    "    ds: int,\n",
    "    scale_lo: float,\n",
    "    scale_hi: float,\n",
    "    write_compression: Optional[str],\n",
    ") -> Tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    Stream TRUE 3D mean pooling over (ds, ds, ds) and write to TiffWriter `tw`.\n",
    "    Returns (D_out, H_out, W_out).\n",
    "    - XY are cropped to multiples of ds for exact block reduce.\n",
    "    - Z uses grouped averaging; the last group may be smaller than ds (handled correctly).\n",
    "    \"\"\"\n",
    "    # Probe shape\n",
    "    s0 = tiff.imread(str(slices[0]))\n",
    "    if s0.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D slices; got shape={s0.shape}\")\n",
    "    H, W = s0.shape\n",
    "    D = len(slices)\n",
    "\n",
    "    if ds <= 1:\n",
    "        # No downsampling; write scaled u8 directly\n",
    "        for p in slices:\n",
    "            arr = tiff.imread(str(p)).astype(np.float32, copy=False)\n",
    "            u8 = _scale_to_u8(arr, scale_lo, scale_hi)\n",
    "            tw.write(u8, photometric=\"minisblack\", compression=write_compression, contiguous=False)\n",
    "        return D, H, W\n",
    "\n",
    "    # Crop XY to multiples of ds for exact block reduce\n",
    "    HH = (H // ds) * ds\n",
    "    WW = (W // ds) * ds\n",
    "    if HH == 0 or WW == 0:\n",
    "        # Too small → fallback to no downsample\n",
    "        for p in slices:\n",
    "            arr = tiff.imread(str(p)).astype(np.float32, copy=False)\n",
    "            u8 = _scale_to_u8(arr, scale_lo, scale_hi)\n",
    "            tw.write(u8, photometric=\"minisblack\", compression=write_compression, contiguous=False)\n",
    "        return D, H, W\n",
    "\n",
    "    H_out, W_out = HH // ds, WW // ds\n",
    "    acc = np.zeros((H_out, W_out), dtype=np.uint32)  # accumulates spatial block sums across Z\n",
    "    zcount = 0\n",
    "    D_out = 0\n",
    "\n",
    "    for i, p in enumerate(slices, start=1):\n",
    "        arr = tiff.imread(str(p)).astype(np.float32, copy=False)\n",
    "        u8 = _scale_to_u8(arr, scale_lo, scale_hi)\n",
    "        acc += _pool2d_sum_u8(u8, ds)\n",
    "        zcount += 1\n",
    "\n",
    "        # When we collected ds slices, or at the tail, emit one pooled slice\n",
    "        if zcount == ds or i == len(slices):\n",
    "            denom = ds * ds * zcount\n",
    "            out = ((acc.astype(np.float32) / float(denom)) + 0.5).astype(np.uint8, copy=False)\n",
    "            tw.write(out, photometric=\"minisblack\", compression=write_compression, contiguous=False)\n",
    "            acc.fill(0)\n",
    "            zcount = 0\n",
    "            D_out += 1\n",
    "\n",
    "    return D_out, H_out, W_out\n",
    "\n",
    "\n",
    "# -------------------- main converter --------------------\n",
    "def convert_manifest_sequences_to_single_tifs(\n",
    "    manifest_path: str | Path,\n",
    "    out_dir: str | Path,\n",
    "    *,\n",
    "    filter_query: Optional[str] = None,        # e.g. \"n_slices >= 128 and size_8bit_GB < 6\"\n",
    "    id_prefix: str = \"S\",\n",
    "    start_id: int = 1,\n",
    "    overwrite: bool = False,\n",
    "    sample_slices_for_range: int = 64,\n",
    "    range_percentiles: Tuple[float, float] = (1.0, 99.9),\n",
    "    bigtiff_threshold_gb: float = 3.5,         # BigTIFF if projected size exceeds this\n",
    "    write_compression: Optional[str] = None,   # e.g. \"zlib\" (smaller, slower). None = fastest\n",
    "    excel_name: str = \"single_tif_index.xlsx\",\n",
    "\n",
    "    # Size-based TRUE 3D downsampling policy\n",
    "    ds_thresholds_gb: Tuple[float, float] = (0.4, 1.0),   # (>0.4→2×, >1.0→4×)\n",
    "    ds_factors: Tuple[int, int] = (2, 4),                 # corresponding factors\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Convert per-scan TIFF slice folders (from a manifest) into single 8-bit multi-page TIFFs.\n",
    "    Adds automatic TRUE 3D downsampling via mean pooling (ds×ds×ds) when large.\n",
    "\n",
    "      if size_8bit_GB > ds_thresholds_gb[1] -> ds_factors[1] (default 4×)\n",
    "      elif size_8bit_GB > ds_thresholds_gb[0] -> ds_factors[0] (default 2×)\n",
    "      else -> 1×\n",
    "\n",
    "    'size_8bit_GB' is taken from the manifest if present; otherwise estimated as D*H*W / 1e9 (u8).\n",
    "    \"\"\"\n",
    "    manifest_path = Path(manifest_path)\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load manifest\n",
    "    if manifest_path.suffix.lower() == \".parquet\":\n",
    "        df = pd.read_parquet(manifest_path)\n",
    "    else:\n",
    "        df = pd.read_csv(manifest_path)\n",
    "\n",
    "    if filter_query:\n",
    "        try:\n",
    "            df = df.query(filter_query, engine=\"python\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Invalid filter_query: {filter_query}\\n{e}\")\n",
    "\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"No rows after filtering—nothing to convert.\")\n",
    "\n",
    "    # Required columns\n",
    "    required = {\"root_dir\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Manifest is missing required columns: {missing}\")\n",
    "\n",
    "    next_id = int(start_id)\n",
    "    records: List[Dict] = []\n",
    "\n",
    "    thr_lo, thr_hi = ds_thresholds_gb\n",
    "    f2, f4 = ds_factors\n",
    "\n",
    "    # Process scans\n",
    "    for _, row in tqdm(df.reset_index(drop=True).iterrows(), total=len(df), desc=\"Converting scans\"):\n",
    "        scan_dir = Path(row[\"root_dir\"])\n",
    "        if not scan_dir.exists():\n",
    "            warnings.warn(f\"[skip] Missing scan folder: {scan_dir}\")\n",
    "            continue\n",
    "\n",
    "        slices = _list_slices(scan_dir)\n",
    "        if len(slices) == 0:\n",
    "            warnings.warn(f\"[skip] No tif slices in: {scan_dir}\")\n",
    "            continue\n",
    "\n",
    "        # Shape probe\n",
    "        s0 = tiff.imread(str(slices[0]))\n",
    "        if s0.ndim != 2:\n",
    "            warnings.warn(f\"[skip] First slice not 2D in: {scan_dir} (shape={s0.shape})\")\n",
    "            continue\n",
    "        H, W = s0.shape\n",
    "        D = len(slices)\n",
    "\n",
    "        # lo/hi (from manifest if present, else estimate)\n",
    "        lo = float(row[\"lo\"]) if \"lo\" in row and pd.notna(row[\"lo\"]) else None\n",
    "        hi = float(row[\"hi\"]) if \"hi\" in row and pd.notna(row[\"hi\"]) else None\n",
    "        if lo is None or hi is None:\n",
    "            lo, hi = _estimate_lo_hi_from_sample(\n",
    "                slices, sample_slices=sample_slices_for_range, percentiles=range_percentiles\n",
    "            )\n",
    "\n",
    "        # Size and downsample factor\n",
    "        if \"size_8bit_GB\" in row and pd.notna(row[\"size_8bit_GB\"]):\n",
    "            size_gb_8bit = float(row[\"size_8bit_GB\"])\n",
    "        else:\n",
    "            size_gb_8bit = (D * H * W) / 1e9\n",
    "\n",
    "        if size_gb_8bit > thr_hi:\n",
    "            ds = int(f4)\n",
    "        elif size_gb_8bit > thr_lo:\n",
    "            ds = int(f2)\n",
    "        else:\n",
    "            ds = 1\n",
    "\n",
    "        # Predict post-downsample dims/sizes (for BigTIFF decision)\n",
    "        if ds <= 1:\n",
    "            D_out_pred, H_out_pred, W_out_pred = D, H, W\n",
    "        else:\n",
    "            D_out_pred = math.ceil(D / ds)\n",
    "            H_out_pred = H // ds\n",
    "            W_out_pred = W // ds\n",
    "\n",
    "        projected_gb_after = (D_out_pred * H_out_pred * W_out_pred) / 1e9\n",
    "        bigtiff_flag = projected_gb_after > bigtiff_threshold_gb\n",
    "\n",
    "        # Output path\n",
    "        scan_id = f\"{id_prefix}{next_id:04d}\"\n",
    "        out_path = out_dir / f\"{scan_id}.tif\"\n",
    "        next_id += 1\n",
    "\n",
    "        if out_path.exists() and not overwrite:\n",
    "            warnings.warn(f\"[skip] Exists: {out_path} (use overwrite=True to rewrite)\")\n",
    "            # record mapping even when skipped\n",
    "            rec = dict(row)\n",
    "            rec.update({\n",
    "                \"scan_id\": scan_id,\n",
    "                \"output_tif\": str(out_path),\n",
    "                \"out_H\": int(H_out_pred), \"out_W\": int(W_out_pred), \"out_D\": int(D_out_pred),\n",
    "                \"used_lo\": lo, \"used_hi\": hi,\n",
    "                \"ds_factor\": ds,\n",
    "                \"projected_gb_before\": size_gb_8bit,\n",
    "                \"projected_gb_after\": projected_gb_after,\n",
    "                \"status\": \"skipped_exists\",\n",
    "            })\n",
    "            records.append(rec)\n",
    "            continue\n",
    "\n",
    "        # Write with TRUE 3D downsampling (streamed)\n",
    "        try:\n",
    "            with tiff.TiffWriter(str(out_path), bigtiff=bigtiff_flag) as tw:\n",
    "                D_written, H_written, W_written = _downsample3d_streaming_write(\n",
    "                    slices=slices,\n",
    "                    tw=tw,\n",
    "                    ds=ds,\n",
    "                    scale_lo=lo,\n",
    "                    scale_hi=hi,\n",
    "                    write_compression=write_compression,\n",
    "                )\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"[error] Failed writing {out_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        rec = dict(row)\n",
    "        rec.update({\n",
    "            \"scan_id\": scan_id,\n",
    "            \"output_tif\": str(out_path),\n",
    "            \"out_H\": int(H_written), \"out_W\": int(W_written), \"out_D\": int(D_written),\n",
    "            \"used_lo\": lo, \"used_hi\": hi,\n",
    "            \"ds_factor\": ds,\n",
    "            \"projected_gb_before\": size_gb_8bit,\n",
    "            \"projected_gb_after\": projected_gb_after,\n",
    "            \"status\": \"ok\",\n",
    "        })\n",
    "        records.append(rec)\n",
    "\n",
    "    # Build Excel index\n",
    "    if not records:\n",
    "        raise RuntimeError(\"No scans converted—check your filter or manifest.\")\n",
    "\n",
    "    df_out = pd.DataFrame(records)\n",
    "    excel_path = out_dir / excel_name\n",
    "    with pd.ExcelWriter(excel_path, engine=\"xlsxwriter\") as xw:\n",
    "        df_out.to_excel(xw, index=False, sheet_name=\"index\")\n",
    "\n",
    "    print(f\"\\nDone. Wrote {len(df_out[df_out['status']=='ok'])} scans to: {out_dir}\")\n",
    "    print(f\"Excel index: {excel_path}\")\n",
    "    return excel_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f430fe1-ef42-4dea-9999-586b7852b64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting scans: 100%|██████████████████████████████████████████████████████████| 467/467 [16:42:56<00:00, 128.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. Wrote 467 scans to: T:\\users\\taki\\Dataset_L\n",
      "Excel index: T:\\users\\taki\\Dataset_L\\index_single_8bit.xlsx\n",
      "T:\\users\\taki\\Dataset_L\\index_single_8bit.xlsx\n"
     ]
    }
   ],
   "source": [
    "# -------------------- example usage (comment/uncomment) --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    excel_index = convert_manifest_sequences_to_single_tifs(\n",
    "        manifest_path=r\"C:\\Users\\taki\\DeepDeBlur3D\\manifest.parquet\",\n",
    "        out_dir=r\"T:\\users\\taki\\Dataset_L\",\n",
    "        filter_query=None,  # e.g. \"size_8bit_GB >= 0.4\"\n",
    "        id_prefix=\"S\",\n",
    "        start_id=1,\n",
    "        overwrite=False,\n",
    "        sample_slices_for_range=64,\n",
    "        range_percentiles=(1.0, 99.9),\n",
    "        bigtiff_threshold_gb=3.5,\n",
    "        write_compression=None,  # or \"zlib\"\n",
    "        ds_thresholds_gb=(0.4, 1.0),\n",
    "        ds_factors=(2, 4),\n",
    "        excel_name=\"index_single_8bit.xlsx\",\n",
    "    )\n",
    "    print(excel_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1bbbbc1-54c9-45c1-8382-366dbe516adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Excel → C:\\Users\\taki\\DeepDeBlur3D\\manifest_export.xlsx\n"
     ]
    }
   ],
   "source": [
    "# convert_manifest_to_excel.py\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def convert_manifest_to_excel(\n",
    "    manifest_path: str | os.PathLike,\n",
    "    out_xlsx: str | os.PathLike,\n",
    "    filter_query: str | None = None,\n",
    "    select_cols: list[str] | None = None,\n",
    "    sort_by: list[str] | None = None,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Convert a deblur3d manifest (parquet/csv) to an Excel workbook.\n",
    "    \n",
    "    - filter_query: optional pandas query string, e.g. \"n_slices>=128 and size_8bit_GB<6\"\n",
    "    - select_cols: optional list of columns to keep (in this order)\n",
    "    - sort_by:     optional list of columns to sort by (ascending)\n",
    "    \"\"\"\n",
    "    mp = Path(manifest_path)\n",
    "    if not mp.exists():\n",
    "        raise FileNotFoundError(f\"Manifest not found: {mp}\")\n",
    "\n",
    "    # Load\n",
    "    if mp.suffix.lower() == \".parquet\":\n",
    "        df = pd.read_parquet(mp)\n",
    "    else:\n",
    "        df = pd.read_csv(mp)\n",
    "\n",
    "    # Filter\n",
    "    if filter_query:\n",
    "        df = df.query(filter_query, engine=\"python\").copy()\n",
    "\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"No rows after filtering; nothing to write.\")\n",
    "\n",
    "    # Add derived sizes if available (bytes → GB for readability)\n",
    "    # Keep existing columns if already present.\n",
    "    if {\"n_slices\",\"H\",\"W\"}.issubset(df.columns) and \"size_8bit_GB\" not in df.columns:\n",
    "        df[\"size_8bit_GB\"] = (df[\"n_slices\"].astype(\"float64\") * df[\"H\"] * df[\"W\"]) / 1e9\n",
    "\n",
    "    if \"xml_bytes_8bit_from_xml\" in df.columns and \"size_8bit_GB_xml\" not in df.columns:\n",
    "        df[\"size_8bit_GB_xml\"] = df[\"xml_bytes_8bit_from_xml\"] / 1e9\n",
    "\n",
    "    # Column selection\n",
    "    if select_cols:\n",
    "        keep = [c for c in select_cols if c in df.columns]\n",
    "        df = df[keep]\n",
    "\n",
    "    # Sorting\n",
    "    if sort_by:\n",
    "        sort_cols = [c for c in sort_by if c in df.columns]\n",
    "        if sort_cols:\n",
    "            df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    # Summary sheet\n",
    "    summary = {\n",
    "        \"rows\": [len(df)],\n",
    "        \"unique_projects\": [df[\"project_path\"].nunique() if \"project_path\" in df.columns else None],\n",
    "        \"total_size_8bit_GB\": [df[\"size_8bit_GB\"].sum() if \"size_8bit_GB\" in df.columns else None],\n",
    "        \"total_size_8bit_GB_xml\": [df[\"size_8bit_GB_xml\"].sum() if \"size_8bit_GB_xml\" in df.columns else None],\n",
    "        \"mean_H\": [df[\"H\"].mean() if \"H\" in df.columns else None],\n",
    "        \"mean_W\": [df[\"W\"].mean() if \"W\" in df.columns else None],\n",
    "        \"mean_n_slices\": [df[\"n_slices\"].mean() if \"n_slices\" in df.columns else None],\n",
    "    }\n",
    "    df_summary = pd.DataFrame(summary)\n",
    "\n",
    "    # Write Excel\n",
    "    out = Path(out_xlsx)\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with pd.ExcelWriter(out, engine=\"xlsxwriter\") as writer:\n",
    "        df.to_excel(writer, sheet_name=\"Scans\", index=False)\n",
    "        df_summary.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "\n",
    "        # Optional: add a simple table style\n",
    "        wb  = writer.book\n",
    "        ws1 = writer.sheets[\"Scans\"]\n",
    "        ws2 = writer.sheets[\"Summary\"]\n",
    "        if len(df) > 0:\n",
    "            ws1.add_table(0, 0, len(df), max(0, len(df.columns)-1),\n",
    "                          {\"name\": \"ScansTable\", \"columns\": [{\"header\": c} for c in df.columns]})\n",
    "        ws2.add_table(0, 0, len(df_summary), max(0, len(df_summary.columns)-1),\n",
    "                      {\"name\": \"SummaryTable\", \"columns\": [{\"header\": c} for c in df_summary.columns]})\n",
    "        # Autofit-ish: set column widths\n",
    "        for i, c in enumerate(df.columns):\n",
    "            width = min(60, max(10, int(df[c].astype(str).str.len().quantile(0.95)) + 2))\n",
    "            ws1.set_column(i, i, width)\n",
    "        for i, c in enumerate(df_summary.columns):\n",
    "            ws2.set_column(i, i, max(12, len(c) + 2))\n",
    "\n",
    "    print(f\"Saved Excel → {out}\")\n",
    "    return out\n",
    "\n",
    "# ===== example usage =====\n",
    "if __name__ == \"__main__\":\n",
    "    manifest = r\"C:\\Users\\taki\\DeepDeBlur3D\\manifest.parquet\"\n",
    "    out_xlsx = r\"C:\\Users\\taki\\DeepDeBlur3D\\manifest_export.xlsx\"\n",
    "\n",
    "    convert_manifest_to_excel(\n",
    "        manifest_path=manifest,\n",
    "        out_xlsx=out_xlsx,\n",
    "        filter_query=None,  # e.g. \"n_slices>=128 and size_8bit_GB<6\"\n",
    "        select_cols=[\n",
    "            \"volume_id\",\"project_path\",\"root_dir\",\"n_slices\",\"H\",\"W\",\"dtype\",\n",
    "            \"lo\",\"hi\",\"spacing_y\",\"spacing_x\",\"size_8bit_GB\",\"size_8bit_GB_xml\"\n",
    "        ],\n",
    "        sort_by=[\"project_path\",\"root_dir\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aacfa0f-0fdc-4b84-b3ee-997858bde021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deblur3d)",
   "language": "python",
   "name": "deblur3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
